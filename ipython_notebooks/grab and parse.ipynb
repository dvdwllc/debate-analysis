{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change this to the actual url and use requests to pull the page\n",
    "index = \"http://www.presidency.ucsb.edu/debates.php\"\n",
    "response = requests.get(index)\n",
    "\n",
    "# beautiful soup object for html parsing\n",
    "soup = BeautifulSoup(response.text.encode('utf-8'))\n",
    "\n",
    "# find all links to debate transcripts\n",
    "debate_links = soup.find_all('td', attrs={'class':'doctext'})\n",
    "debate_dates = soup.find_all('td', attrs={'class':'docdate'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#transcript_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got 127 urls, 20 listings with no link.\n"
     ]
    }
   ],
   "source": [
    "# get urls for each transcript\n",
    "transcript_urls = []\n",
    "n_sans_link = 0\n",
    "for i in range(len(debate_links)):\n",
    "    try:\n",
    "        transcript_urls.append(\n",
    "            (debate_links[i].select('a')[0].get('href'), \n",
    "             debate_dates[i].text.split(', ')[1])\n",
    "        )\n",
    "    except IndexError:\n",
    "        n_sans_link += 1\n",
    "        \n",
    "print 'got %i urls, %i listings with no link.' % (len(transcript_urls), n_sans_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.presidency.ucsb.edu/ws/index.php?pid=97332'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_urls[-16][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#response2.text[89135:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "response1 = requests.get(transcript_urls[0][0])\n",
    "response2 = requests.get(transcript_urls[-16][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# html starts after a fat block of javascript and XML.\n",
    "# read the page after that point\n",
    "\n",
    "# new pages have the participants names in bold\n",
    "start1 = response1.text.index('<span class=\"displaytext\">')\n",
    "end1 = response1.text.index('<hr noshade=\"noshade\" size=\"1\">')\n",
    "soup1 = BeautifulSoup(response1.text.encode('utf-8'))\n",
    "\n",
    "# old pages do not bold the participants names\n",
    "start2 = response2.text.index('<span class=\"displaytext\">')\n",
    "end2 = response2.text.index('<hr noshade=\"noshade\" size=\"1\">')\n",
    "soup2 = BeautifulSoup(response2.text[start2:end2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.4.1'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "bs4.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#soup1.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_speakers_and_quotes(page, year):\n",
    "    \"\"\"\n",
    "    Extracts names of every person involved with the debate and \n",
    "    collects all of their quotes in a list.\n",
    "    \n",
    "    :param page: html page\n",
    "    \n",
    "    :returns dict: {speaker name: [quotes]}\n",
    "    \"\"\"\n",
    "    #print year\n",
    "    # fixes a BeautifulSoup problem that results in improper parsing\n",
    "    page_soup = BeautifulSoup(page.text.encode('utf-8'))\n",
    "    \n",
    "    speaker_dict = dict()\n",
    "\n",
    "    prev_speaker = ''\n",
    "    for i in page_soup.find_all('p'):\n",
    "\n",
    "        try: # search for name of person speaking\n",
    "            curr_speaker = re.findall('[A-Z]+:', i.text)[0].strip(':')\n",
    "            prev_speaker = curr_speaker\n",
    "            quote = re.split(':', i.text)[1]\n",
    "\n",
    "        except IndexError: # if name not in line\n",
    "            quote = i.text\n",
    "\n",
    "        if prev_speaker+'_'+year not in speaker_dict:\n",
    "            # add speaker to speaker_dict with list of quotes as value\n",
    "            speaker_dict[prev_speaker+'_'+year] = [quote]\n",
    "        else:\n",
    "            # append quote to speaker's list of quotes\n",
    "            speaker_dict[prev_speaker+'_'+year].append(quote)\n",
    "            \n",
    "    return speaker_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from requests_futures.sessions import FuturesSession\n",
    "\n",
    "npages = len(transcript_urls)\n",
    "\n",
    "session = FuturesSession(max_workers=5)\n",
    "futures = [session.get(url[0]) for url in transcript_urls[:npages]]\n",
    "\n",
    "dates = [url[1] for url in transcript_urls[:npages]]\n",
    "results = zip(futures, dates)\n",
    "\n",
    "debate_transcripts = [parse_speakers_and_quotes(future.result(), date) for future, date in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dill.dump(debate_transcripts, open('debate_transcripts_list.dill', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#debate_transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
